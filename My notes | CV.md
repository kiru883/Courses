# Backbones / classification
  - ResNet
  - VGG16
## Models
  ### ResNet
![](https://github.com/kiru883/Courses/blob/master/CV%20notes/images/resnet.PNG)

Основная мысль - res. block(fig. c) вместо того чтоб аппроксимироваться к H(x)("идеальный" feature map), с помощью классических методов, апроксимируемся "остатками". 
Н(х) = F(x) + x, где х это инпут, F(x) это остаток, каждый блок использует пропуск соединения(shortcut), мое интуитивное понятие, ассоциация с град. бустингом, 
там мы тоже генерим ОСТАТКИ градиентов лосса и затем суммируем их с предиктом на начальном предикторе.
- Fig. A - основные архитектуры реснет сетей, конв блоки имеют same паддинг
- Fig. C - основной блок этой сетки, bottleneck, их  2 вида, первый для сетей до 34 слоев, второй для сеток после 34 слоев, линия сбоку - shortcut(identity mapping). 
В настоящих реализациях после конв. используется пакетная нормализация и активация.
- Fig. B - пример 34-й архитектуры, разными цветами обозначены блоки с РАЗНЫМ размером feature map'a, используется также 2 вида shortcut'a, первый - классический, обычная передача инпута, второй для изменения размера feature map'a, изменять он его может двумя разными методами: А - непараметр. метод, проходимся пулингом 1х1 с страйдом 2, размер feature map'ов изменился(уменьшился) но количество каналов такое же, добавляем нулевые feature map'ы. В - параметрический метод, он лучше, проходимся конв. 1х1 с страйдом 2(к примеру на изображении было 64 канала, прошлись 128 ядрами с страйдом 2)

Ссылки
  - https://arxiv.org/pdf/1409.1556.pdf - ориг
  - https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py - реализация
  - https://shuzhanfan.github.io/2018/11/ResNet/ - тут утверждают, что relu лучше применять ток к остаткам
  - https://towardsdatascience.com/intuition-behind-residual-neural-networks-fa5d2996b2c7
  - https://neurohive.io/ru/vidy-nejrosetej/resnet-34-50-101/



# Segmentation
  - Unet
    - https://arxiv.org/pdf/1505.04597.pdf
